# 1. Abstractだけ読む
Learning to Understand Child-directed and Adult-directed Speech  
https://www.aclweb.org/anthology/2020.acl-main.1/  
子供向けの発話内容(CDS)をNLPタスクの学習に使うと, 最終的な性能は大人向けの発話内容(ADS)に劣るが, 学習初期の性能はADSより高くなる

(MEDICAL)Predicting Depression in Screening Interviews from Latent Categorization of Interview Prompts  
(MEDICAL)https://www.aclweb.org/anthology/2020.acl-main.2/  
(MEDICAL)精神科の問診の文字起こしテキストからのうつ病の予測を行うJLPCモデルを提案. 患者の応答をベクトルにEncodeする際, 直前の医師の質問のトピックのカテゴリを推測しそれも考慮するようにした

Coach: A Coarse-to-Fine Approach for Cross-domain Slot Filling  
https://www.aclweb.org/anthology/2020.acl-main.3/  
対話システムに用いるSlot Filling modelの学習データ量をcross-domain slot fillingによって減らす手法について, 多段階で推論を行う新手法Coachを提案. 対話システムのみならずCross-domain NERでもSOTAを更新した

Designing Precise and Robust Dialogue Response Evaluators  
https://www.aclweb.org/anthology/2020.acl-main.4/  
対話システムの新たな評価手法を提案. 従来Human judgementと比較していたところを, 新手法ではMLMを用いてreferenceなしでHuman judgementとの相関係数>0.6を達成

Dialogue State Tracking with Explicit Slot Connection Modeling  
https://www.aclweb.org/anthology/2020.acl-main.5/  
対話システムにおけるdialogue state tracking(DST)を改善する新手法DST-SCを提案. MultiWOZ 2.0, 2.1 datasetでSOTAを更新

Generating Informative Conversational Response using Recurrent Knowledge-Interaction and Knowledge-Copy  
https://www.aclweb.org/anthology/2020.acl-main.6/  
知識データベースをもとにrecommendationなどを行う対話システムについて, 会話の自然さを保ちつつ適切な情報提供を行えるような手法を提案. Wizard-of-Wikipedia, DuConvの2つのデータセットでSOTAを更新

Guiding Variational Response Generator to Exploit Persona  
https://www.aclweb.org/anthology/2020.acl-main.7/  
対話システムにおいて, ユーザーの人物像を考慮した返答を行うNeural Reponse Generators(NRG)を改善する新手法を提案, さらに3つの新たなpersona-orientedな評価手法も提案した

Large Scale Multi-Actor Generative Dialog Modeling  
https://www.aclweb.org/anthology/2020.acl-main.8/  
複数名からなる雑談を生成するとき, それぞれの登場人物のpersonalityや文体を有効に保つために各人物の過去の発話履歴を活用する手法を提案した

PLATO: Pre-trained Dialogue Generation Model with Discrete Latent Variable  
https://www.aclweb.org/anthology/2020.acl-main.9/  
Transformerを用いて対話生成モデルの事前学習を行う一般的な枠組みを提案. Context, Act(隠れ変数), Recognitionからなるグラフィカルモデルを利用

Slot-consistent NLG for Task-oriented Dialogue Systems with Iterative Rectification Network  
https://www.aclweb.org/anthology/2020.acl-main.10/  
Data-drivenな自然言語生成を行う際のいわゆるhallucination phenomenon (=入力されたslot valueの一部が生成文から欠落したり誤ったキーとして扱われたりする現象) の解決を試みた研究. 誤生成例に対して強化学習を用いたIterative Rectification Network (IRN) による書き換えを行うことで正確性と流暢さを向上させた

Span-ConveRT: Few-shot Span Extraction for Dialog with Pretrained Conversational Representations  
https://www.aclweb.org/anthology/2020.acl-main.11/  
Data-drivenな対話システムにおけるslot-filling taskをfew-shotでも行えるようにした研究. Turn-based span-extraction taskと見立てた場合にRESTAURANTS-8k datasetにおいてSpanBERTやCNN-CRFの性能を上回った. モデル構造はConveRT+CNN+CRF

Zero-Shot Transfer Learning with Synthesized Data for Multi-Domain Dialogue State Tracking  
https://www.aclweb.org/anthology/2020.acl-main.12/  
その名の通り他domainに対してzero-shotでDSTを行う研究. まずstate, agent act, user act, next stateの組をテンプレートとして約30種類定義しているのが特徴(abstract dialogue model). MultiWOZ datasetにおいてzero-shot taskのSOTAを更新

A Complete Shift-Reduce Chinese Discourse Parser with Robust Dynamic Oracle  
https://www.aclweb.org/anthology/2020.acl-main.13/  
中国語を対象としたend-to-endなdiscourse parserを提案.

TransS-Driven Joint Learning Architecture for Implicit Discourse Relation Recognition   
https://www.aclweb.org/anthology/2020.acl-main.14/   
談話関係認識(=2つの節や文の論理的な関係を認識する)タスクにおいて, 2つの入力のsemantic featureとdiscourse relationを同時に学習する手法を提案.

A Study of Non-autoregressive Model for Sequence Generation  
https://www.aclweb.org/anthology/2020.acl-main.15/  
生成系のタスクにおいて, 一般にnon-autoregressive(NAR) modelはautoregressive(AR) modelより動作が速い反面出力文の品質が落ちるが, 特にTarget-token間の依存性が高いタスクほどそうなりやすいことを示し, NAR modelにとってのタスクの難易度を推定するCoMMA modelを提唱した  
コメント: アブストがとても読みやすい

Cross-modal Language Generation using Pivot Stabilization for Web-scale Language Coverage  
https://www.aclweb.org/anthology/2020.acl-main.16/  
★英語以外で書かれたキャプション生成用データセットが少ない問題を解決しようとした研究. もとの英語キャプションと対象言語に機械翻訳したキャプションを学習データとし, 英語キャプションと対象言語のキャプションをconcatした文を生成させるタスクを解く手法 PLuGS を提案した. 結果, 対象言語での性能が向上しただけでなく, 英語キャプションのほうの生成の質も向上した

Fact-based Text Editing  
https://www.aclweb.org/anthology/2020.acl-main.17/  
新たなdata-to-text taskを提案した. 修正前と修正後の生成文の組とtripletが与えられ, tripletの知識を過不足なく含むように修正前の文を書き換える. 既存のdata-to-textデータセットから自動でfact-based text editingタスクデータセットを2つ作成したほか, FactEditorというモデルを提案し, encoder-decoderよりも高速かつ高性能を実現

Few-Shot NLG with Pre-Trained Language Model   
https://www.aclweb.org/anthology/2020.acl-main.18/  
★事前学習済みGPT-2を使ってdata-to-textタスクを解く研究. 言語モデルはそのまま使い, tableの読み取り(table encoder)と生成の機構(switch policy)のほうに工夫を加えているのが特徴

Fluent Response Generation for Conversational Question Answering  
https://www.aclweb.org/anthology/2020.acl-main.19/ 
会話形式のQAタスク(conversational QA, ConvQA)をspan extractionではなく生成で解く研究. ConvQAでは解答を完全な文で出力する必要があるが, QAデータセットの正答は文の形では与えられていない場合もある. そこで, 既存のQAデータセットの正答を完全な文となるように書き換えたデータセットを自動的に作成し, それをseq2seqタスクとして解くことにより高い性能を実現  

Diverse and Consistent QA pairs from Contexts with Information-Maximizing Hierarchical Conditional VAEs  
https://www.aclweb.org/anthology/2020.acl-main.20/  
構造化されていないテキストからQAペアを自動的かつ大量に生成する方法を提案.  

Learning to Ask More: Semi-Autoregressive Sequential Question Generation under Dual-Graph Interaction  
https://www.aclweb.org/anthology/2020.acl-main.21/  
構造化されていないテキストから一連のQAペアを自動的に作成し, かつ質問文が省略を含んでおり単体では解答できないが前の質問文を参照すれば解答できるようなものとなる, Sequential Question Generation (SQG) に関する研究. 従来は対話タスクの手法が一般的だったが, semi-autoregressiveに解くことで質問間の繋がりがより自然になった

Neural Syntactic Preordering for Controlled Paraphrase Generation  
https://www.aclweb.org/anthology/2020.acl-main.22/  
言い換えタスクに新手法を導入した研究. まず入力文を構文解析し, その構文木をencoder-decoderによって組み替える. 次に入力文の語順をどのように入れ替えれば組み替え後の構文木が実現されるかを割り出し, positional embeddingをその順序で入れ替えたうえでTransformerベースのencoder-decoderに入力文を与えるアプローチをとった

Pre-train and Plug-in: Flexible Conditional Text Generation with Variational Auto-Encoders  
https://www.aclweb.org/anthology/2020.acl-main.23/  
★自然言語生成にVAEを導入した研究. これにより生成時にさまざまな条件づけが可能となった

Probabilistically Masked Language Model Capable of Autoregressive Generation in Arbitrary Word Order  
https://www.aclweb.org/anthology/2020.acl-main.24/  
★Masked Language Modelで高性能な自然言語生成を行うため, 事前学習時にマスクするトークンの割合を一様分布からランダムに決定するu-PMLMを提案. u-PMLMの学習がautoregressiveなTransformerの学習を任意の生成順序に一般化したものであることを数学的に示し, 実際にGPTより高性能で自然言語生成ができることを示した


